{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b0IUcC6HyOFN"
      },
      "source": [
        "# MusicVAE for Drum\n",
        "\n",
        "Groove MIDI Dataset을 학습하여 4마디의 드럼 연주 샘플을 만드는 과정을 보이는 예시입니다.  \n",
        "이 노트북은 Colab환경에서 작성되었습니다.  \n",
        "\n",
        "## DATASET\n",
        "[Groove MIDI Dataset](https://magenta.tensorflow.org/datasets/groove)  \n",
        "- TF DATASET에 존재하는 데이터이므로 TFDS모듈을 통해서 사용했습니다.  \n",
        "\n",
        "## Model\n",
        "[MusicVAE](https://arxiv.org/pdf/1803.05428.pdf) 논문의 모델을 구현한  \n",
        "[Magenta](https://github.com/magenta/magenta/tree/master/magenta/models/music_vae)의 MUSIC_VAE를 활용하여 구성했습니다.  \n",
        "\n",
        "colab환경에서 큰 모델을 구성하여 학습시키기에는 무리가 있다고 생각해 작은 모델을 구현하였으나, 상황에 따라 바꿀 수 있게끔 작성하였습니다.   \n",
        "\n",
        "### Encoder\n",
        "논문에서와 같이 **2개**의 **BidirectionalLSTM** 레이어를 사용하였고, 크기는 **512**입니다.  \n",
        "\n",
        "latent 차원은 **256**을 지정했습니다.  \n",
        "\n",
        "논문에서는 2048 size의 BidirectionalLSTM 레이어 2개와 512차원의 latent vector를 사용했습니다.  \n",
        "\n",
        "### Decoder\n",
        "논문의 핵심이 **계층적 디코더**를 활용하여 시퀀스의 길이가 긴 구조에 대해서 VAE모델을 만드는 것이라 생각해 4마디의 샘플을 만드는 모델이지만 계층적 LSTM을 활용해서 구현했습니다.  \n",
        "\n",
        "디코더 역시 **2개**의 **CategoricalLstmDecoder** 레이어를 사용하였고 그 크기는 **256**입니다.  \n",
        "\n",
        "논문에서는 1024크기의 디코더를 사용했습니다.  \n",
        "\n",
        "### Others\n",
        "DrumsConverter를 사용하였고,  \n",
        "\n",
        "다른 파라미터는 `music_vae.configs`에 있는 다른 모델들을 참고하였습니다.  \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0uMjP_AU_fsc"
      },
      "source": [
        "## 환경 구성\n",
        "모델과 학습에 필요한 magenta 라이브러리와 음악 재생에 필요한 모듈들을 다운로드 합니다.  \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VYiIABEIZAEP",
        "outputId": "79420d5c-cf7b-4744-b5dc-4ce6eab695ef"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Selecting previously unselected package fluid-soundfont-gm.\n",
            "(Reading database ... 155632 files and directories currently installed.)\n",
            "Preparing to unpack .../fluid-soundfont-gm_3.1-5.1_all.deb ...\n",
            "Unpacking fluid-soundfont-gm (3.1-5.1) ...\n",
            "Selecting previously unselected package libfluidsynth1:amd64.\n",
            "Preparing to unpack .../libfluidsynth1_1.1.9-1_amd64.deb ...\n",
            "Unpacking libfluidsynth1:amd64 (1.1.9-1) ...\n",
            "Setting up fluid-soundfont-gm (3.1-5.1) ...\n",
            "Setting up libfluidsynth1:amd64 (1.1.9-1) ...\n",
            "Processing triggers for libc-bin (2.27-3ubuntu1.3) ...\n",
            "/sbin/ldconfig.real: /usr/local/lib/python3.7/dist-packages/ideep4py/lib/libmkldnn.so.0 is not a symbolic link\n",
            "\n",
            "Cloning into 'magenta'...\n",
            "remote: Enumerating objects: 15848, done.\u001b[K\n",
            "remote: Total 15848 (delta 0), reused 0 (delta 0), pack-reused 15848\u001b[K\n",
            "Receiving objects: 100% (15848/15848), 36.37 MiB | 25.49 MiB/s, done.\n",
            "Resolving deltas: 100% (12054/12054), done.\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting pretty_midi\n",
            "  Downloading pretty_midi-0.2.9.tar.gz (5.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 5.6 MB 26.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.7.0 in /usr/local/lib/python3.7/dist-packages (from pretty_midi) (1.21.6)\n",
            "Collecting mido>=1.1.16\n",
            "  Downloading mido-1.2.10-py2.py3-none-any.whl (51 kB)\n",
            "\u001b[K     |████████████████████████████████| 51 kB 8.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from pretty_midi) (1.15.0)\n",
            "Building wheels for collected packages: pretty-midi\n",
            "  Building wheel for pretty-midi (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pretty-midi: filename=pretty_midi-0.2.9-py3-none-any.whl size=5591955 sha256=3b8bb05177425d00cec9402e89542d03fda2aa84e40db13241f850379c8d2c69\n",
            "  Stored in directory: /root/.cache/pip/wheels/ad/74/7c/a06473ca8dcb63efb98c1e67667ce39d52100f837835ea18fa\n",
            "Successfully built pretty-midi\n",
            "Installing collected packages: mido, pretty-midi\n",
            "Successfully installed mido-1.2.10 pretty-midi-0.2.9\n"
          ]
        }
      ],
      "source": [
        "!apt-get update -qq && apt-get install -qq libfluidsynth1 fluid-soundfont-gm build-essential libasound2-dev libjack-dev\n",
        "!pip install -q pyfluidsynth\n",
        "!git clone https://github.com/tensorflow/magenta.git\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v-lB7L-eZVoO",
        "outputId": "794afd6f-da3f-48f9-c99d-015d29aabbd2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content/magenta\n"
          ]
        }
      ],
      "source": [
        "cd /content/magenta"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EahlQC0HZHUg",
        "outputId": "9ed44317-7277-4dea-ee6f-0eb09e34a756"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Obtaining file:///content/magenta\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.7/dist-packages (from magenta==2.1.3) (1.0.0)\n",
            "Collecting dm-sonnet\n",
            "  Downloading dm_sonnet-2.0.0-py3-none-any.whl (254 kB)\n",
            "\u001b[K     |████████████████████████████████| 254 kB 22.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: dopamine-rl<=3.0.1 in /usr/local/lib/python3.7/dist-packages (from magenta==2.1.3) (1.0.5)\n",
            "Requirement already satisfied: imageio in /usr/local/lib/python3.7/dist-packages (from magenta==2.1.3) (2.4.1)\n",
            "Collecting librosa<0.8.0,>=0.6.2\n",
            "  Downloading librosa-0.7.2.tar.gz (1.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.6 MB 45.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: matplotlib>=1.5.3 in /usr/local/lib/python3.7/dist-packages (from magenta==2.1.3) (3.2.2)\n",
            "Collecting mido==1.2.6\n",
            "  Downloading mido-1.2.6-py2.py3-none-any.whl (69 kB)\n",
            "\u001b[K     |████████████████████████████████| 69 kB 8.6 MB/s \n",
            "\u001b[?25hCollecting mir_eval>=0.4\n",
            "  Downloading mir_eval-0.7.tar.gz (90 kB)\n",
            "\u001b[K     |████████████████████████████████| 90 kB 10.8 MB/s \n",
            "\u001b[?25hCollecting note-seq\n",
            "  Downloading note_seq-0.0.3-py3-none-any.whl (210 kB)\n",
            "\u001b[K     |████████████████████████████████| 210 kB 42.9 MB/s \n",
            "\u001b[?25hCollecting numba<0.50\n",
            "  Downloading numba-0.49.1-cp37-cp37m-manylinux2014_x86_64.whl (3.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.6 MB 9.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from magenta==2.1.3) (1.21.6)\n",
            "Requirement already satisfied: Pillow>=3.4.2 in /usr/local/lib/python3.7/dist-packages (from magenta==2.1.3) (7.1.2)\n",
            "Requirement already satisfied: pretty_midi>=0.2.6 in /usr/local/lib/python3.7/dist-packages (from magenta==2.1.3) (0.2.9)\n",
            "Collecting pygtrie>=2.3\n",
            "  Downloading pygtrie-2.4.2.tar.gz (35 kB)\n",
            "Collecting python-rtmidi<1.2,>=1.1\n",
            "  Downloading python-rtmidi-1.1.2.tar.gz (204 kB)\n",
            "\u001b[K     |████████████████████████████████| 204 kB 64.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: scikit-image in /usr/local/lib/python3.7/dist-packages (from magenta==2.1.3) (0.18.3)\n",
            "Requirement already satisfied: scipy>=0.18.1 in /usr/local/lib/python3.7/dist-packages (from magenta==2.1.3) (1.4.1)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.7/dist-packages (from magenta==2.1.3) (1.15.0)\n",
            "Collecting sk-video\n",
            "  Downloading sk_video-1.1.10-py2.py3-none-any.whl (2.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 2.3 MB 43.0 MB/s \n",
            "\u001b[?25hCollecting sox>=1.3.7\n",
            "  Downloading sox-1.4.1-py2.py3-none-any.whl (39 kB)\n",
            "Collecting tensor2tensor\n",
            "  Downloading tensor2tensor-1.15.7-py2.py3-none-any.whl (1.4 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.4 MB 35.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: gym<0.22.0 in /usr/local/lib/python3.7/dist-packages (from magenta==2.1.3) (0.17.3)\n",
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.7/dist-packages (from magenta==2.1.3) (2.8.2+zzzcolab20220527125636)\n",
            "Requirement already satisfied: tensorflow-datasets in /usr/local/lib/python3.7/dist-packages (from magenta==2.1.3) (4.0.1)\n",
            "Requirement already satisfied: tensorflow-probability in /usr/local/lib/python3.7/dist-packages (from magenta==2.1.3) (0.16.0)\n",
            "Collecting tf_slim\n",
            "  Downloading tf_slim-1.1.0-py2.py3-none-any.whl (352 kB)\n",
            "\u001b[K     |████████████████████████████████| 352 kB 56.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: wheel in /usr/local/lib/python3.7/dist-packages (from magenta==2.1.3) (0.37.1)\n",
            "Requirement already satisfied: gin-config>=0.1.1 in /usr/local/lib/python3.7/dist-packages (from dopamine-rl<=3.0.1->magenta==2.1.3) (0.5.0)\n",
            "Requirement already satisfied: opencv-python>=3.4.1.15 in /usr/local/lib/python3.7/dist-packages (from dopamine-rl<=3.0.1->magenta==2.1.3) (4.1.2.30)\n",
            "Requirement already satisfied: cloudpickle<1.7.0,>=1.2.0 in /usr/local/lib/python3.7/dist-packages (from gym<0.22.0->magenta==2.1.3) (1.3.0)\n",
            "Requirement already satisfied: pyglet<=1.5.0,>=1.4.0 in /usr/local/lib/python3.7/dist-packages (from gym<0.22.0->magenta==2.1.3) (1.5.0)\n",
            "Requirement already satisfied: audioread>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from librosa<0.8.0,>=0.6.2->magenta==2.1.3) (2.1.9)\n",
            "Requirement already satisfied: scikit-learn!=0.19.0,>=0.14.0 in /usr/local/lib/python3.7/dist-packages (from librosa<0.8.0,>=0.6.2->magenta==2.1.3) (1.0.2)\n",
            "Requirement already satisfied: joblib>=0.12 in /usr/local/lib/python3.7/dist-packages (from librosa<0.8.0,>=0.6.2->magenta==2.1.3) (1.1.0)\n",
            "Requirement already satisfied: decorator>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from librosa<0.8.0,>=0.6.2->magenta==2.1.3) (4.4.2)\n",
            "Requirement already satisfied: resampy>=0.2.2 in /usr/local/lib/python3.7/dist-packages (from librosa<0.8.0,>=0.6.2->magenta==2.1.3) (0.2.2)\n",
            "Requirement already satisfied: soundfile>=0.9.0 in /usr/local/lib/python3.7/dist-packages (from librosa<0.8.0,>=0.6.2->magenta==2.1.3) (0.10.3.post1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=1.5.3->magenta==2.1.3) (1.4.2)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=1.5.3->magenta==2.1.3) (2.8.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=1.5.3->magenta==2.1.3) (0.11.0)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=1.5.3->magenta==2.1.3) (3.0.9)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from kiwisolver>=1.0.1->matplotlib>=1.5.3->magenta==2.1.3) (4.2.0)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from mir_eval>=0.4->magenta==2.1.3) (0.16.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from numba<0.50->magenta==2.1.3) (57.4.0)\n",
            "Collecting llvmlite<=0.33.0.dev0,>=0.31.0.dev0\n",
            "  Downloading llvmlite-0.32.1-cp37-cp37m-manylinux1_x86_64.whl (20.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 20.2 MB 1.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn!=0.19.0,>=0.14.0->librosa<0.8.0,>=0.6.2->magenta==2.1.3) (3.1.0)\n",
            "Requirement already satisfied: cffi>=1.0 in /usr/local/lib/python3.7/dist-packages (from soundfile>=0.9.0->librosa<0.8.0,>=0.6.2->magenta==2.1.3) (1.15.0)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.7/dist-packages (from cffi>=1.0->soundfile>=0.9.0->librosa<0.8.0,>=0.6.2->magenta==2.1.3) (2.21)\n",
            "Requirement already satisfied: tabulate>=0.7.5 in /usr/local/lib/python3.7/dist-packages (from dm-sonnet->magenta==2.1.3) (0.8.9)\n",
            "Requirement already satisfied: dm-tree>=0.1.1 in /usr/local/lib/python3.7/dist-packages (from dm-sonnet->magenta==2.1.3) (0.1.7)\n",
            "Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.7/dist-packages (from dm-sonnet->magenta==2.1.3) (1.14.1)\n",
            "Requirement already satisfied: intervaltree>=2.1.0 in /usr/local/lib/python3.7/dist-packages (from note-seq->magenta==2.1.3) (2.1.0)\n",
            "Collecting pydub\n",
            "  Downloading pydub-0.25.1-py2.py3-none-any.whl (32 kB)\n",
            "Requirement already satisfied: attrs in /usr/local/lib/python3.7/dist-packages (from note-seq->magenta==2.1.3) (21.4.0)\n",
            "Requirement already satisfied: IPython in /usr/local/lib/python3.7/dist-packages (from note-seq->magenta==2.1.3) (5.5.0)\n",
            "Requirement already satisfied: bokeh>=0.12.0 in /usr/local/lib/python3.7/dist-packages (from note-seq->magenta==2.1.3) (2.3.3)\n",
            "Requirement already satisfied: pandas>=0.18.1 in /usr/local/lib/python3.7/dist-packages (from note-seq->magenta==2.1.3) (1.3.5)\n",
            "Requirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.7/dist-packages (from note-seq->magenta==2.1.3) (3.17.3)\n",
            "Requirement already satisfied: Jinja2>=2.9 in /usr/local/lib/python3.7/dist-packages (from bokeh>=0.12.0->note-seq->magenta==2.1.3) (2.11.3)\n",
            "Requirement already satisfied: packaging>=16.8 in /usr/local/lib/python3.7/dist-packages (from bokeh>=0.12.0->note-seq->magenta==2.1.3) (21.3)\n",
            "Requirement already satisfied: tornado>=5.1 in /usr/local/lib/python3.7/dist-packages (from bokeh>=0.12.0->note-seq->magenta==2.1.3) (5.1.1)\n",
            "Requirement already satisfied: PyYAML>=3.10 in /usr/local/lib/python3.7/dist-packages (from bokeh>=0.12.0->note-seq->magenta==2.1.3) (3.13)\n",
            "Requirement already satisfied: sortedcontainers in /usr/local/lib/python3.7/dist-packages (from intervaltree>=2.1.0->note-seq->magenta==2.1.3) (2.4.0)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from Jinja2>=2.9->bokeh>=0.12.0->note-seq->magenta==2.1.3) (2.0.1)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=0.18.1->note-seq->magenta==2.1.3) (2022.1)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.7/dist-packages (from IPython->note-seq->magenta==2.1.3) (2.6.1)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.7/dist-packages (from IPython->note-seq->magenta==2.1.3) (0.7.5)\n",
            "Requirement already satisfied: pexpect in /usr/local/lib/python3.7/dist-packages (from IPython->note-seq->magenta==2.1.3) (4.8.0)\n",
            "Requirement already satisfied: traitlets>=4.2 in /usr/local/lib/python3.7/dist-packages (from IPython->note-seq->magenta==2.1.3) (5.1.1)\n",
            "Requirement already satisfied: simplegeneric>0.8 in /usr/local/lib/python3.7/dist-packages (from IPython->note-seq->magenta==2.1.3) (0.8.1)\n",
            "Requirement already satisfied: prompt-toolkit<2.0.0,>=1.0.4 in /usr/local/lib/python3.7/dist-packages (from IPython->note-seq->magenta==2.1.3) (1.0.18)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.7/dist-packages (from prompt-toolkit<2.0.0,>=1.0.4->IPython->note-seq->magenta==2.1.3) (0.2.5)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.7/dist-packages (from pexpect->IPython->note-seq->magenta==2.1.3) (0.7.0)\n",
            "Requirement already satisfied: tifffile>=2019.7.26 in /usr/local/lib/python3.7/dist-packages (from scikit-image->magenta==2.1.3) (2021.11.2)\n",
            "Requirement already satisfied: PyWavelets>=1.1.1 in /usr/local/lib/python3.7/dist-packages (from scikit-image->magenta==2.1.3) (1.3.0)\n",
            "Requirement already satisfied: networkx>=2.0 in /usr/local/lib/python3.7/dist-packages (from scikit-image->magenta==2.1.3) (2.6.3)\n",
            "Collecting tensorflow-probability\n",
            "  Downloading tensorflow_probability-0.7.0-py2.py3-none-any.whl (981 kB)\n",
            "\u001b[K     |████████████████████████████████| 981 kB 52.2 MB/s \n",
            "\u001b[?25hCollecting gunicorn\n",
            "  Downloading gunicorn-20.1.0-py3-none-any.whl (79 kB)\n",
            "\u001b[K     |████████████████████████████████| 79 kB 10.0 MB/s \n",
            "\u001b[?25hCollecting pypng\n",
            "  Downloading pypng-0.0.21-py3-none-any.whl (48 kB)\n",
            "\u001b[K     |████████████████████████████████| 48 kB 6.3 MB/s \n",
            "\u001b[?25hCollecting bz2file\n",
            "  Downloading bz2file-0.98.tar.gz (11 kB)\n",
            "Collecting mesh-tensorflow\n",
            "  Downloading mesh_tensorflow-0.1.21-py3-none-any.whl (385 kB)\n",
            "\u001b[K     |████████████████████████████████| 385 kB 57.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from tensor2tensor->magenta==2.1.3) (4.64.0)\n",
            "Collecting gevent\n",
            "  Downloading gevent-21.12.0-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (5.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 5.8 MB 51.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from tensor2tensor->magenta==2.1.3) (2.23.0)\n",
            "Collecting kfac\n",
            "  Downloading kfac-0.2.4-py2.py3-none-any.whl (193 kB)\n",
            "\u001b[K     |████████████████████████████████| 193 kB 51.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: oauth2client in /usr/local/lib/python3.7/dist-packages (from tensor2tensor->magenta==2.1.3) (4.1.3)\n",
            "Requirement already satisfied: flask in /usr/local/lib/python3.7/dist-packages (from tensor2tensor->magenta==2.1.3) (1.1.4)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.7/dist-packages (from tensor2tensor->magenta==2.1.3) (3.1.0)\n",
            "Collecting tensorflow-gan\n",
            "  Downloading tensorflow_gan-2.1.0-py2.py3-none-any.whl (367 kB)\n",
            "\u001b[K     |████████████████████████████████| 367 kB 50.8 MB/s \n",
            "\u001b[?25hCollecting tensorflow-addons\n",
            "  Downloading tensorflow_addons-0.17.0-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.1 MB 49.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: google-api-python-client in /usr/local/lib/python3.7/dist-packages (from tensor2tensor->magenta==2.1.3) (1.12.11)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.7/dist-packages (from tensor2tensor->magenta==2.1.3) (1.7.1)\n",
            "Requirement already satisfied: click<8.0,>=5.1 in /usr/local/lib/python3.7/dist-packages (from flask->tensor2tensor->magenta==2.1.3) (7.1.2)\n",
            "Requirement already satisfied: Werkzeug<2.0,>=0.15 in /usr/local/lib/python3.7/dist-packages (from flask->tensor2tensor->magenta==2.1.3) (1.0.1)\n",
            "Requirement already satisfied: itsdangerous<2.0,>=0.24 in /usr/local/lib/python3.7/dist-packages (from flask->tensor2tensor->magenta==2.1.3) (1.1.0)\n",
            "Collecting zope.event\n",
            "  Downloading zope.event-4.5.0-py2.py3-none-any.whl (6.8 kB)\n",
            "Requirement already satisfied: greenlet<2.0,>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from gevent->tensor2tensor->magenta==2.1.3) (1.1.2)\n",
            "Collecting zope.interface\n",
            "  Downloading zope.interface-5.4.0-cp37-cp37m-manylinux2010_x86_64.whl (251 kB)\n",
            "\u001b[K     |████████████████████████████████| 251 kB 58.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: google-auth<3dev,>=1.16.0 in /usr/local/lib/python3.7/dist-packages (from google-api-python-client->tensor2tensor->magenta==2.1.3) (1.35.0)\n",
            "Requirement already satisfied: google-auth-httplib2>=0.0.3 in /usr/local/lib/python3.7/dist-packages (from google-api-python-client->tensor2tensor->magenta==2.1.3) (0.0.4)\n",
            "Requirement already satisfied: google-api-core<3dev,>=1.21.0 in /usr/local/lib/python3.7/dist-packages (from google-api-python-client->tensor2tensor->magenta==2.1.3) (1.31.6)\n",
            "Requirement already satisfied: httplib2<1dev,>=0.15.0 in /usr/local/lib/python3.7/dist-packages (from google-api-python-client->tensor2tensor->magenta==2.1.3) (0.17.4)\n",
            "Requirement already satisfied: uritemplate<4dev,>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from google-api-python-client->tensor2tensor->magenta==2.1.3) (3.0.1)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0dev,>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from google-api-core<3dev,>=1.21.0->google-api-python-client->tensor2tensor->magenta==2.1.3) (1.56.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<3dev,>=1.16.0->google-api-python-client->tensor2tensor->magenta==2.1.3) (0.2.8)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<3dev,>=1.16.0->google-api-python-client->tensor2tensor->magenta==2.1.3) (4.2.4)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<3dev,>=1.16.0->google-api-python-client->tensor2tensor->magenta==2.1.3) (4.8)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3dev,>=1.16.0->google-api-python-client->tensor2tensor->magenta==2.1.3) (0.4.8)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->tensor2tensor->magenta==2.1.3) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->tensor2tensor->magenta==2.1.3) (2022.5.18.1)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->tensor2tensor->magenta==2.1.3) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->tensor2tensor->magenta==2.1.3) (3.0.4)\n",
            "Requirement already satisfied: cached-property in /usr/local/lib/python3.7/dist-packages (from h5py->tensor2tensor->magenta==2.1.3) (1.5.2)\n",
            "Collecting h5py\n",
            "  Downloading h5py-2.10.0-cp37-cp37m-manylinux1_x86_64.whl (2.9 MB)\n",
            "\u001b[K     |████████████████████████████████| 2.9 MB 48.0 MB/s \n",
            "\u001b[?25hCollecting kfac\n",
            "  Downloading kfac-0.2.3-py2.py3-none-any.whl (191 kB)\n",
            "\u001b[K     |████████████████████████████████| 191 kB 75.8 MB/s \n",
            "\u001b[?25h  Downloading kfac-0.2.2-py2.py3-none-any.whl (191 kB)\n",
            "\u001b[K     |████████████████████████████████| 191 kB 49.5 MB/s \n",
            "\u001b[?25h  Downloading kfac-0.2.0-py2.py3-none-any.whl (178 kB)\n",
            "\u001b[K     |████████████████████████████████| 178 kB 37.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.7/dist-packages (from sympy->tensor2tensor->magenta==2.1.3) (1.2.1)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow->magenta==2.1.3) (1.1.0)\n",
            "Requirement already satisfied: libclang>=9.0.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow->magenta==2.1.3) (14.0.1)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow->magenta==2.1.3) (0.2.0)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow->magenta==2.1.3) (3.3.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow->magenta==2.1.3) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=1.12 in /usr/local/lib/python3.7/dist-packages (from tensorflow->magenta==2.1.3) (2.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.7/dist-packages (from tensorflow->magenta==2.1.3) (1.46.3)\n",
            "Requirement already satisfied: tensorboard<2.9,>=2.8 in /usr/local/lib/python3.7/dist-packages (from tensorflow->magenta==2.1.3) (2.8.0)\n",
            "Requirement already satisfied: keras<2.9,>=2.8.0rc0 in /usr/local/lib/python3.7/dist-packages (from tensorflow->magenta==2.1.3) (2.8.0)\n",
            "Requirement already satisfied: keras-preprocessing>=1.1.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow->magenta==2.1.3) (1.1.2)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow->magenta==2.1.3) (0.26.0)\n",
            "Requirement already satisfied: tensorflow-estimator<2.9,>=2.8 in /usr/local/lib/python3.7/dist-packages (from tensorflow->magenta==2.1.3) (2.8.0)\n",
            "Requirement already satisfied: gast>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow->magenta==2.1.3) (0.5.3)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow->magenta==2.1.3) (1.8.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow->magenta==2.1.3) (3.3.7)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow->magenta==2.1.3) (0.6.1)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow->magenta==2.1.3) (0.4.6)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.9,>=2.8->tensorflow->magenta==2.1.3) (1.3.1)\n",
            "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard<2.9,>=2.8->tensorflow->magenta==2.1.3) (4.11.4)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<2.9,>=2.8->tensorflow->magenta==2.1.3) (3.8.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.9,>=2.8->tensorflow->magenta==2.1.3) (3.2.0)\n",
            "Requirement already satisfied: typeguard>=2.7 in /usr/local/lib/python3.7/dist-packages (from tensorflow-addons->tensor2tensor->magenta==2.1.3) (2.7.1)\n",
            "Requirement already satisfied: importlib-resources in /usr/local/lib/python3.7/dist-packages (from tensorflow-datasets->magenta==2.1.3) (5.7.1)\n",
            "Requirement already satisfied: dill in /usr/local/lib/python3.7/dist-packages (from tensorflow-datasets->magenta==2.1.3) (0.3.5.1)\n",
            "Requirement already satisfied: tensorflow-metadata in /usr/local/lib/python3.7/dist-packages (from tensorflow-datasets->magenta==2.1.3) (1.8.0)\n",
            "Requirement already satisfied: promise in /usr/local/lib/python3.7/dist-packages (from tensorflow-datasets->magenta==2.1.3) (2.3)\n",
            "Requirement already satisfied: tensorflow-hub>=0.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gan->tensor2tensor->magenta==2.1.3) (0.12.0)\n",
            "Building wheels for collected packages: librosa, mir-eval, pygtrie, python-rtmidi, bz2file\n",
            "  Building wheel for librosa (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for librosa: filename=librosa-0.7.2-py3-none-any.whl size=1612902 sha256=627e0ab0f6b7eeedc6ddf61dc3bd116f47787f3e1b8e408351d1a90dd9240728\n",
            "  Stored in directory: /root/.cache/pip/wheels/18/9e/42/3224f85730f92fa2925f0b4fb6ef7f9c5431a64dfc77b95b39\n",
            "  Building wheel for mir-eval (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for mir-eval: filename=mir_eval-0.7-py3-none-any.whl size=100721 sha256=e337c79d291177fc8548954e3f5191dce3791ff495c830b23e8cc33f34ebbba1\n",
            "  Stored in directory: /root/.cache/pip/wheels/18/5a/46/d2527ff1fd975e1a793375e6ed763bfe4d3ea396b7cdc470eb\n",
            "  Building wheel for pygtrie (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pygtrie: filename=pygtrie-2.4.2-py3-none-any.whl size=19063 sha256=4800b85f1d91f3ccd08a9626e29a23e818670273f8cdacfcf3bfb4d2bb2f4c58\n",
            "  Stored in directory: /root/.cache/pip/wheels/d3/f8/ba/1d828b1603ea422686eb694253a43cb3a5901ea4696c1e0603\n",
            "  Building wheel for python-rtmidi (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for python-rtmidi: filename=python_rtmidi-1.1.2-cp37-cp37m-linux_x86_64.whl size=391738 sha256=18e2f110c68a14f879c9f6ab76c85ddd21702759f88d0a0840cd9700b9889cd3\n",
            "  Stored in directory: /root/.cache/pip/wheels/ba/0c/e1/ccca9af1590f715067166c3199f6b639a26a152f61d4e79397\n",
            "  Building wheel for bz2file (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for bz2file: filename=bz2file-0.98-py3-none-any.whl size=6882 sha256=c3af3bac57ab38b7cf1e8ca22f678ba9ee429e7a8c60b02d40e2037cbe9e7eb4\n",
            "  Stored in directory: /root/.cache/pip/wheels/85/ce/8d/b5f76b602b16a8a39f2ded74189cf5f09fc4a87bea16c54a8b\n",
            "Successfully built librosa mir-eval pygtrie python-rtmidi bz2file\n",
            "Installing collected packages: llvmlite, numba, zope.interface, zope.event, tensorflow-probability, mido, tf-slim, tensorflow-gan, tensorflow-addons, pypng, pydub, mesh-tensorflow, librosa, kfac, gunicorn, gevent, bz2file, tensor2tensor, sox, sk-video, python-rtmidi, pygtrie, note-seq, mir-eval, dm-sonnet, magenta\n",
            "  Attempting uninstall: llvmlite\n",
            "    Found existing installation: llvmlite 0.34.0\n",
            "    Uninstalling llvmlite-0.34.0:\n",
            "      Successfully uninstalled llvmlite-0.34.0\n",
            "  Attempting uninstall: numba\n",
            "    Found existing installation: numba 0.51.2\n",
            "    Uninstalling numba-0.51.2:\n",
            "      Successfully uninstalled numba-0.51.2\n",
            "  Attempting uninstall: tensorflow-probability\n",
            "    Found existing installation: tensorflow-probability 0.16.0\n",
            "    Uninstalling tensorflow-probability-0.16.0:\n",
            "      Successfully uninstalled tensorflow-probability-0.16.0\n",
            "  Attempting uninstall: mido\n",
            "    Found existing installation: mido 1.2.10\n",
            "    Uninstalling mido-1.2.10:\n",
            "      Successfully uninstalled mido-1.2.10\n",
            "  Attempting uninstall: librosa\n",
            "    Found existing installation: librosa 0.8.1\n",
            "    Uninstalling librosa-0.8.1:\n",
            "      Successfully uninstalled librosa-0.8.1\n",
            "  Running setup.py develop for magenta\n",
            "Successfully installed bz2file-0.98 dm-sonnet-2.0.0 gevent-21.12.0 gunicorn-20.1.0 kfac-0.2.0 librosa-0.7.2 llvmlite-0.32.1 magenta-2.1.3 mesh-tensorflow-0.1.21 mido-1.2.6 mir-eval-0.7 note-seq-0.0.3 numba-0.49.1 pydub-0.25.1 pygtrie-2.4.2 pypng-0.0.21 python-rtmidi-1.1.2 sk-video-1.1.10 sox-1.4.1 tensor2tensor-1.15.7 tensorflow-addons-0.17.0 tensorflow-gan-2.1.0 tensorflow-probability-0.7.0 tf-slim-1.1.0 zope.event-4.5.0 zope.interface-5.4.0\n"
          ]
        }
      ],
      "source": [
        "!pip install -e ."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gGubfwvSZIaZ",
        "outputId": "0f407cfc-a9d0-44a8-e5d5-4ef7b48fd075"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/librosa/util/decorators.py:9: NumbaDeprecationWarning: An import was requested from a module that has moved location.\n",
            "Import requested from: 'numba.decorators', please update to use 'numba.core.decorators' or pin to Numba version 0.48.0. This alias will not be present in Numba version 0.50.0.\n",
            "  from numba.decorators import jit as optional_jit\n",
            "/usr/local/lib/python3.7/dist-packages/librosa/util/decorators.py:9: NumbaDeprecationWarning: An import was requested from a module that has moved location.\n",
            "Import of 'jit' requested from: 'numba.decorators', please update to use 'numba.core.decorators' or pin to Numba version 0.48.0. This alias will not be present in Numba version 0.50.0.\n",
            "  from numba.decorators import jit as optional_jit\n"
          ]
        }
      ],
      "source": [
        "from google.colab import files\n",
        "import os\n",
        "import warnings\n",
        "import magenta.music as mm\n",
        "from magenta.models.music_vae import configs\n",
        "from magenta.models.music_vae.trained_model import TrainedModel\n",
        "\n",
        "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "22vfFiIoADxv"
      },
      "source": [
        "## 모델 생성\n",
        "magenta에서는 console을 통한 학습을 권장하는데, 모델에 대한 정보를 `music_vae.config`에서 \n",
        "불러와 학습합니다.  \n",
        "그러나 config를 추가하는 함수를 찾지 못해서 config.py파일에 직접 config를 추가하는 방법을 사용했습니다.  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "oxh53yqsdTzY"
      },
      "outputs": [],
      "source": [
        "config_to_add = \"\"\"\n",
        "CONFIG_MAP['hierdec-drums_4bar'] = Config(\n",
        "    model=MusicVAE(\n",
        "        lstm_models.BidirectionalLstmEncoder(),\n",
        "        lstm_models.HierarchicalLstmDecoder(\n",
        "            lstm_models.CategoricalLstmDecoder(),\n",
        "            level_lengths=[16, 4],\n",
        "            disable_autoregression=True)),\n",
        "    hparams=merge_hparams(\n",
        "        lstm_models.get_default_hparams(),\n",
        "        HParams(\n",
        "            batch_size=512,\n",
        "            max_seq_len=64,\n",
        "            z_size=256,\n",
        "            enc_rnn_size=[512, 512],\n",
        "            dec_rnn_size=[256, 256],\n",
        "            free_bits=48,\n",
        "            max_beta=0.2,\n",
        "            sampling_schedule='inverse_sigmoid',\n",
        "            sampling_rate=1000\n",
        "        )),\n",
        "    note_sequence_augmenter=None,\n",
        "    data_converter=data.DrumsConverter(\n",
        "        max_bars=100,\n",
        "        slice_bars=4,\n",
        "        steps_per_quarter=4,\n",
        "        roll_input=True,\n",
        "    ),\n",
        "    train_examples_path=None,\n",
        "    eval_examples_path=None,\n",
        ")\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "ewovnEJvhMXN"
      },
      "outputs": [],
      "source": [
        "config_file = '/content/magenta/magenta/models/music_vae/configs.py'\n",
        "\n",
        "with open(config_file, 'a') as f:\n",
        "    f.write(config_to_add)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2-yu1t5tBmZB"
      },
      "source": [
        "## 학습\n",
        "학습 스크립트를 통해 학습합니다.  \n",
        "실습환경이므로 학습은 10회만 진행하였습니다.  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iGLxrcbidmlS",
        "outputId": "f795584d-c1cd-4a45-ec61-6e7c708fa7e8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/librosa/util/decorators.py:9: NumbaDeprecationWarning: An import was requested from a module that has moved location.\n",
            "Import requested from: 'numba.decorators', please update to use 'numba.core.decorators' or pin to Numba version 0.48.0. This alias will not be present in Numba version 0.50.0.\n",
            "  from numba.decorators import jit as optional_jit\n",
            "/usr/local/lib/python3.7/dist-packages/librosa/util/decorators.py:9: NumbaDeprecationWarning: An import was requested from a module that has moved location.\n",
            "Import of 'jit' requested from: 'numba.decorators', please update to use 'numba.core.decorators' or pin to Numba version 0.48.0. This alias will not be present in Numba version 0.50.0.\n",
            "  from numba.decorators import jit as optional_jit\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow/python/compat/v2_compat.py:107: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "non-resource variables are not supported in the long term\n",
            "2022-06-05 13:37:22.522250: E tensorflow/stream_executor/cuda/cuda_driver.cc:271] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\n",
            "INFO:tensorflow:Building MusicVAE model with BidirectionalLstmEncoder, HierarchicalLstmDecoder, and hparams:\n",
            "{'max_seq_len': 64, 'z_size': 256, 'free_bits': 48, 'max_beta': 0.2, 'beta_rate': 0.0, 'batch_size': 512, 'grad_clip': 1.0, 'clip_mode': 'global_norm', 'grad_norm_clip_to_zero': 10000, 'learning_rate': 0.001, 'decay_rate': 0.9999, 'min_learning_rate': 1e-05, 'conditional': True, 'dec_rnn_size': [256, 256], 'enc_rnn_size': [512, 512], 'dropout_keep_prob': 1.0, 'sampling_schedule': 'inverse_sigmoid', 'sampling_rate': 1000, 'use_cudnn': False, 'residual_encoder': False, 'residual_decoder': False, 'control_preprocessing_rnn_size': [256]}\n",
            "I0605 13:37:22.672557 140255853238144 base_model.py:152] Building MusicVAE model with BidirectionalLstmEncoder, HierarchicalLstmDecoder, and hparams:\n",
            "{'max_seq_len': 64, 'z_size': 256, 'free_bits': 48, 'max_beta': 0.2, 'beta_rate': 0.0, 'batch_size': 512, 'grad_clip': 1.0, 'clip_mode': 'global_norm', 'grad_norm_clip_to_zero': 10000, 'learning_rate': 0.001, 'decay_rate': 0.9999, 'min_learning_rate': 1e-05, 'conditional': True, 'dec_rnn_size': [256, 256], 'enc_rnn_size': [512, 512], 'dropout_keep_prob': 1.0, 'sampling_schedule': 'inverse_sigmoid', 'sampling_rate': 1000, 'use_cudnn': False, 'residual_encoder': False, 'residual_decoder': False, 'control_preprocessing_rnn_size': [256]}\n",
            "INFO:tensorflow:\n",
            "Encoder Cells (bidirectional):\n",
            "  units: [512, 512]\n",
            "\n",
            "I0605 13:37:22.678392 140255853238144 lstm_models.py:78] \n",
            "Encoder Cells (bidirectional):\n",
            "  units: [512, 512]\n",
            "\n",
            "WARNING:tensorflow:`tf.nn.rnn_cell.MultiRNNCell` is deprecated. This class is equivalent as `tf.keras.layers.StackedRNNCells`, and will be replaced by that in Tensorflow 2.0.\n",
            "W0605 13:37:22.689672 140255853238144 rnn_cell_impl.py:1259] `tf.nn.rnn_cell.MultiRNNCell` is deprecated. This class is equivalent as `tf.keras.layers.StackedRNNCells`, and will be replaced by that in Tensorflow 2.0.\n",
            "WARNING:tensorflow:`tf.nn.rnn_cell.MultiRNNCell` is deprecated. This class is equivalent as `tf.keras.layers.StackedRNNCells`, and will be replaced by that in Tensorflow 2.0.\n",
            "W0605 13:37:22.698865 140255853238144 rnn_cell_impl.py:1259] `tf.nn.rnn_cell.MultiRNNCell` is deprecated. This class is equivalent as `tf.keras.layers.StackedRNNCells`, and will be replaced by that in Tensorflow 2.0.\n",
            "WARNING:tensorflow:`tf.nn.rnn_cell.MultiRNNCell` is deprecated. This class is equivalent as `tf.keras.layers.StackedRNNCells`, and will be replaced by that in Tensorflow 2.0.\n",
            "W0605 13:37:22.712023 140255853238144 rnn_cell_impl.py:1259] `tf.nn.rnn_cell.MultiRNNCell` is deprecated. This class is equivalent as `tf.keras.layers.StackedRNNCells`, and will be replaced by that in Tensorflow 2.0.\n",
            "WARNING:tensorflow:`tf.nn.rnn_cell.MultiRNNCell` is deprecated. This class is equivalent as `tf.keras.layers.StackedRNNCells`, and will be replaced by that in Tensorflow 2.0.\n",
            "W0605 13:37:22.720944 140255853238144 rnn_cell_impl.py:1259] `tf.nn.rnn_cell.MultiRNNCell` is deprecated. This class is equivalent as `tf.keras.layers.StackedRNNCells`, and will be replaced by that in Tensorflow 2.0.\n",
            "INFO:tensorflow:\n",
            "Hierarchical Decoder:\n",
            "  input length: 64\n",
            "  level output lengths: [16, 4]\n",
            "\n",
            "I0605 13:37:22.721456 140255853238144 lstm_models.py:898] \n",
            "Hierarchical Decoder:\n",
            "  input length: 64\n",
            "  level output lengths: [16, 4]\n",
            "\n",
            "WARNING:tensorflow:`tf.nn.rnn_cell.MultiRNNCell` is deprecated. This class is equivalent as `tf.keras.layers.StackedRNNCells`, and will be replaced by that in Tensorflow 2.0.\n",
            "W0605 13:37:22.736536 140255853238144 rnn_cell_impl.py:1259] `tf.nn.rnn_cell.MultiRNNCell` is deprecated. This class is equivalent as `tf.keras.layers.StackedRNNCells`, and will be replaced by that in Tensorflow 2.0.\n",
            "INFO:tensorflow:\n",
            "Decoder Cells:\n",
            "  units: [256, 256]\n",
            "\n",
            "I0605 13:37:22.737159 140255853238144 lstm_models.py:224] \n",
            "Decoder Cells:\n",
            "  units: [256, 256]\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow/python/util/dispatch.py:1082: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.cast` instead.\n",
            "W0605 13:37:22.737379 140255853238144 deprecation.py:343] From /usr/local/lib/python3.7/dist-packages/tensorflow/python/util/dispatch.py:1082: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.cast` instead.\n",
            "WARNING:tensorflow:`tf.nn.rnn_cell.MultiRNNCell` is deprecated. This class is equivalent as `tf.keras.layers.StackedRNNCells`, and will be replaced by that in Tensorflow 2.0.\n",
            "W0605 13:37:22.760083 140255853238144 rnn_cell_impl.py:1259] `tf.nn.rnn_cell.MultiRNNCell` is deprecated. This class is equivalent as `tf.keras.layers.StackedRNNCells`, and will be replaced by that in Tensorflow 2.0.\n",
            "INFO:tensorflow:Reading examples from TFDS: groove/full-midionly\n",
            "I0605 13:37:22.760637 140255853238144 data.py:1832] Reading examples from TFDS: groove/full-midionly\n",
            "2022-06-05 13:37:22.845889: W tensorflow/core/platform/cloud/google_auth_provider.cc:184] All attempts to get a Google authentication bearer token failed, returning an empty token. Retrieving token from files failed with \"NOT_FOUND: Could not locate the credentials file.\". Retrieving token from GCE failed with \"NOT_FOUND: Error executing an HTTP request: HTTP response code 404\".\n",
            "I0605 13:37:23.148032 140255853238144 dataset_info.py:361] Load dataset info from gs://tfds-data/datasets/groove/full-midionly/2.0.1\n",
            "I0605 13:37:23.684882 140255853238144 dataset_info.py:405] Field info.description from disk and from code do not match. Keeping the one from code.\n",
            "I0605 13:37:23.685103 140255853238144 dataset_info.py:405] Field info.config_name from disk and from code do not match. Keeping the one from code.\n",
            "I0605 13:37:23.685196 140255853238144 dataset_info.py:405] Field info.config_description from disk and from code do not match. Keeping the one from code.\n",
            "I0605 13:37:23.685281 140255853238144 dataset_info.py:405] Field info.citation from disk and from code do not match. Keeping the one from code.\n",
            "I0605 13:37:23.731336 140255853238144 dataset_builder.py:299] Reusing dataset groove (gs://tfds-data/datasets/groove/full-midionly/2.0.1)\n",
            "I0605 13:37:23.731586 140255853238144 dataset_builder.py:511] Constructing tf.data.Dataset for split train, from gs://tfds-data/datasets/groove/full-midionly/2.0.1\n",
            "WARNING:tensorflow:From /content/magenta/magenta/contrib/rnn.py:473: bidirectional_dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `keras.layers.Bidirectional(keras.layers.RNN(cell))`, which is equivalent to this API\n",
            "W0605 13:37:24.044833 140255853238144 deprecation.py:343] From /content/magenta/magenta/contrib/rnn.py:473: bidirectional_dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `keras.layers.Bidirectional(keras.layers.RNN(cell))`, which is equivalent to this API\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow/python/ops/rnn.py:446: dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `keras.layers.RNN(cell)`, which is equivalent to this API\n",
            "W0605 13:37:24.045223 140255853238144 deprecation.py:343] From /usr/local/lib/python3.7/dist-packages/tensorflow/python/ops/rnn.py:446: dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `keras.layers.RNN(cell)`, which is equivalent to this API\n",
            "/content/magenta/magenta/contrib/rnn.py:750: UserWarning: `layer.add_variable` is deprecated and will be removed in a future version. Please use `layer.add_weight` method instead.\n",
            "  self._names[\"W\"], [input_size + self._num_units, self._num_units * 4])\n",
            "/content/magenta/magenta/contrib/rnn.py:753: UserWarning: `layer.add_variable` is deprecated and will be removed in a future version. Please use `layer.add_weight` method instead.\n",
            "  initializer=tf.constant_initializer(0.0))\n",
            "/content/magenta/magenta/models/music_vae/base_model.py:199: UserWarning: `tf.layers.dense` is deprecated and will be removed in a future version. Please use `tf.keras.layers.Dense` instead.\n",
            "  kernel_initializer=tf.random_normal_initializer(stddev=0.001))\n",
            "/usr/local/lib/python3.7/dist-packages/keras/legacy_tf_layers/core.py:261: UserWarning: `layer.apply` is deprecated and will be removed in a future version. Please use `layer.__call__` method instead.\n",
            "  return layer.apply(inputs)\n",
            "/content/magenta/magenta/models/music_vae/base_model.py:205: UserWarning: `tf.layers.dense` is deprecated and will be removed in a future version. Please use `tf.keras.layers.Dense` instead.\n",
            "  kernel_initializer=tf.random_normal_initializer(stddev=0.001))\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow_probability/python/bijectors/affine_linear_operator.py:116: LinearOperator.graph_parents (from tensorflow.python.ops.linalg.linear_operator) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Do not call `graph_parents`.\n",
            "W0605 13:37:24.629500 140255853238144 deprecation.py:343] From /usr/local/lib/python3.7/dist-packages/tensorflow_probability/python/bijectors/affine_linear_operator.py:116: LinearOperator.graph_parents (from tensorflow.python.ops.linalg.linear_operator) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Do not call `graph_parents`.\n",
            "/content/magenta/magenta/models/music_vae/lstm_utils.py:99: UserWarning: `tf.layers.dense` is deprecated and will be removed in a future version. Please use `tf.keras.layers.Dense` instead.\n",
            "  name=name),\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow/python/util/dispatch.py:1082: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "\n",
            "Future major versions of TensorFlow will allow gradients to flow\n",
            "into the labels input on backprop by default.\n",
            "\n",
            "See `tf.nn.softmax_cross_entropy_with_logits_v2`.\n",
            "\n",
            "W0605 13:37:25.135868 140255853238144 deprecation.py:343] From /usr/local/lib/python3.7/dist-packages/tensorflow/python/util/dispatch.py:1082: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "\n",
            "Future major versions of TensorFlow will allow gradients to flow\n",
            "into the labels input on backprop by default.\n",
            "\n",
            "See `tf.nn.softmax_cross_entropy_with_logits_v2`.\n",
            "\n",
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/indexed_slices.py:446: UserWarning: Converting sparse IndexedSlices(IndexedSlices(indices=Tensor(\"gradients/core_decoder_1/decoder/while/BasicDecoderStep/ScheduledOutputTrainingHelperNextInputs/cond/GatherNd_1/Switch_grad/cond_grad/Merge_1:0\", shape=(?,), dtype=int32), values=Tensor(\"gradients/core_decoder_1/decoder/while/BasicDecoderStep/ScheduledOutputTrainingHelperNextInputs/cond/GatherNd_1/Switch_grad/cond_grad/Merge:0\", shape=(?, 768), dtype=float32), dense_shape=Tensor(\"gradients/core_decoder_1/decoder/while/BasicDecoderStep/ScheduledOutputTrainingHelperNextInputs/cond/GatherNd_1/Switch_grad/cond_grad/Merge_2:0\", shape=(2,), dtype=int32))) to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
            "  \"shape. This may consume a large amount of memory.\" % value)\n",
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/indexed_slices.py:446: UserWarning: Converting sparse IndexedSlices(IndexedSlices(indices=Tensor(\"gradients/core_decoder_2/decoder/while/BasicDecoderStep/ScheduledOutputTrainingHelperNextInputs/cond/GatherNd_1/Switch_grad/cond_grad/Merge_1:0\", shape=(?,), dtype=int32), values=Tensor(\"gradients/core_decoder_2/decoder/while/BasicDecoderStep/ScheduledOutputTrainingHelperNextInputs/cond/GatherNd_1/Switch_grad/cond_grad/Merge:0\", shape=(?, 768), dtype=float32), dense_shape=Tensor(\"gradients/core_decoder_2/decoder/while/BasicDecoderStep/ScheduledOutputTrainingHelperNextInputs/cond/GatherNd_1/Switch_grad/cond_grad/Merge_2:0\", shape=(2,), dtype=int32))) to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
            "  \"shape. This may consume a large amount of memory.\" % value)\n",
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/indexed_slices.py:446: UserWarning: Converting sparse IndexedSlices(IndexedSlices(indices=Tensor(\"gradients/core_decoder_3/decoder/while/BasicDecoderStep/ScheduledOutputTrainingHelperNextInputs/cond/GatherNd_1/Switch_grad/cond_grad/Merge_1:0\", shape=(?,), dtype=int32), values=Tensor(\"gradients/core_decoder_3/decoder/while/BasicDecoderStep/ScheduledOutputTrainingHelperNextInputs/cond/GatherNd_1/Switch_grad/cond_grad/Merge:0\", shape=(?, 768), dtype=float32), dense_shape=Tensor(\"gradients/core_decoder_3/decoder/while/BasicDecoderStep/ScheduledOutputTrainingHelperNextInputs/cond/GatherNd_1/Switch_grad/cond_grad/Merge_2:0\", shape=(2,), dtype=int32))) to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
            "  \"shape. This may consume a large amount of memory.\" % value)\n",
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/indexed_slices.py:446: UserWarning: Converting sparse IndexedSlices(IndexedSlices(indices=Tensor(\"gradients/core_decoder_4/decoder/while/BasicDecoderStep/ScheduledOutputTrainingHelperNextInputs/cond/GatherNd_1/Switch_grad/cond_grad/Merge_1:0\", shape=(?,), dtype=int32), values=Tensor(\"gradients/core_decoder_4/decoder/while/BasicDecoderStep/ScheduledOutputTrainingHelperNextInputs/cond/GatherNd_1/Switch_grad/cond_grad/Merge:0\", shape=(?, 768), dtype=float32), dense_shape=Tensor(\"gradients/core_decoder_4/decoder/while/BasicDecoderStep/ScheduledOutputTrainingHelperNextInputs/cond/GatherNd_1/Switch_grad/cond_grad/Merge_2:0\", shape=(2,), dtype=int32))) to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
            "  \"shape. This may consume a large amount of memory.\" % value)\n",
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/indexed_slices.py:446: UserWarning: Converting sparse IndexedSlices(IndexedSlices(indices=Tensor(\"gradients/core_decoder_5/decoder/while/BasicDecoderStep/ScheduledOutputTrainingHelperNextInputs/cond/GatherNd_1/Switch_grad/cond_grad/Merge_1:0\", shape=(?,), dtype=int32), values=Tensor(\"gradients/core_decoder_5/decoder/while/BasicDecoderStep/ScheduledOutputTrainingHelperNextInputs/cond/GatherNd_1/Switch_grad/cond_grad/Merge:0\", shape=(?, 768), dtype=float32), dense_shape=Tensor(\"gradients/core_decoder_5/decoder/while/BasicDecoderStep/ScheduledOutputTrainingHelperNextInputs/cond/GatherNd_1/Switch_grad/cond_grad/Merge_2:0\", shape=(2,), dtype=int32))) to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
            "  \"shape. This may consume a large amount of memory.\" % value)\n",
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/indexed_slices.py:446: UserWarning: Converting sparse IndexedSlices(IndexedSlices(indices=Tensor(\"gradients/core_decoder_6/decoder/while/BasicDecoderStep/ScheduledOutputTrainingHelperNextInputs/cond/GatherNd_1/Switch_grad/cond_grad/Merge_1:0\", shape=(?,), dtype=int32), values=Tensor(\"gradients/core_decoder_6/decoder/while/BasicDecoderStep/ScheduledOutputTrainingHelperNextInputs/cond/GatherNd_1/Switch_grad/cond_grad/Merge:0\", shape=(?, 768), dtype=float32), dense_shape=Tensor(\"gradients/core_decoder_6/decoder/while/BasicDecoderStep/ScheduledOutputTrainingHelperNextInputs/cond/GatherNd_1/Switch_grad/cond_grad/Merge_2:0\", shape=(2,), dtype=int32))) to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
            "  \"shape. This may consume a large amount of memory.\" % value)\n",
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/indexed_slices.py:446: UserWarning: Converting sparse IndexedSlices(IndexedSlices(indices=Tensor(\"gradients/core_decoder_7/decoder/while/BasicDecoderStep/ScheduledOutputTrainingHelperNextInputs/cond/GatherNd_1/Switch_grad/cond_grad/Merge_1:0\", shape=(?,), dtype=int32), values=Tensor(\"gradients/core_decoder_7/decoder/while/BasicDecoderStep/ScheduledOutputTrainingHelperNextInputs/cond/GatherNd_1/Switch_grad/cond_grad/Merge:0\", shape=(?, 768), dtype=float32), dense_shape=Tensor(\"gradients/core_decoder_7/decoder/while/BasicDecoderStep/ScheduledOutputTrainingHelperNextInputs/cond/GatherNd_1/Switch_grad/cond_grad/Merge_2:0\", shape=(2,), dtype=int32))) to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
            "  \"shape. This may consume a large amount of memory.\" % value)\n",
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/indexed_slices.py:446: UserWarning: Converting sparse IndexedSlices(IndexedSlices(indices=Tensor(\"gradients/core_decoder_8/decoder/while/BasicDecoderStep/ScheduledOutputTrainingHelperNextInputs/cond/GatherNd_1/Switch_grad/cond_grad/Merge_1:0\", shape=(?,), dtype=int32), values=Tensor(\"gradients/core_decoder_8/decoder/while/BasicDecoderStep/ScheduledOutputTrainingHelperNextInputs/cond/GatherNd_1/Switch_grad/cond_grad/Merge:0\", shape=(?, 768), dtype=float32), dense_shape=Tensor(\"gradients/core_decoder_8/decoder/while/BasicDecoderStep/ScheduledOutputTrainingHelperNextInputs/cond/GatherNd_1/Switch_grad/cond_grad/Merge_2:0\", shape=(2,), dtype=int32))) to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
            "  \"shape. This may consume a large amount of memory.\" % value)\n",
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/indexed_slices.py:446: UserWarning: Converting sparse IndexedSlices(IndexedSlices(indices=Tensor(\"gradients/core_decoder_9/decoder/while/BasicDecoderStep/ScheduledOutputTrainingHelperNextInputs/cond/GatherNd_1/Switch_grad/cond_grad/Merge_1:0\", shape=(?,), dtype=int32), values=Tensor(\"gradients/core_decoder_9/decoder/while/BasicDecoderStep/ScheduledOutputTrainingHelperNextInputs/cond/GatherNd_1/Switch_grad/cond_grad/Merge:0\", shape=(?, 768), dtype=float32), dense_shape=Tensor(\"gradients/core_decoder_9/decoder/while/BasicDecoderStep/ScheduledOutputTrainingHelperNextInputs/cond/GatherNd_1/Switch_grad/cond_grad/Merge_2:0\", shape=(2,), dtype=int32))) to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
            "  \"shape. This may consume a large amount of memory.\" % value)\n",
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/indexed_slices.py:446: UserWarning: Converting sparse IndexedSlices(IndexedSlices(indices=Tensor(\"gradients/core_decoder_10/decoder/while/BasicDecoderStep/ScheduledOutputTrainingHelperNextInputs/cond/GatherNd_1/Switch_grad/cond_grad/Merge_1:0\", shape=(?,), dtype=int32), values=Tensor(\"gradients/core_decoder_10/decoder/while/BasicDecoderStep/ScheduledOutputTrainingHelperNextInputs/cond/GatherNd_1/Switch_grad/cond_grad/Merge:0\", shape=(?, 768), dtype=float32), dense_shape=Tensor(\"gradients/core_decoder_10/decoder/while/BasicDecoderStep/ScheduledOutputTrainingHelperNextInputs/cond/GatherNd_1/Switch_grad/cond_grad/Merge_2:0\", shape=(2,), dtype=int32))) to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
            "  \"shape. This may consume a large amount of memory.\" % value)\n",
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/indexed_slices.py:446: UserWarning: Converting sparse IndexedSlices(IndexedSlices(indices=Tensor(\"gradients/core_decoder_11/decoder/while/BasicDecoderStep/ScheduledOutputTrainingHelperNextInputs/cond/GatherNd_1/Switch_grad/cond_grad/Merge_1:0\", shape=(?,), dtype=int32), values=Tensor(\"gradients/core_decoder_11/decoder/while/BasicDecoderStep/ScheduledOutputTrainingHelperNextInputs/cond/GatherNd_1/Switch_grad/cond_grad/Merge:0\", shape=(?, 768), dtype=float32), dense_shape=Tensor(\"gradients/core_decoder_11/decoder/while/BasicDecoderStep/ScheduledOutputTrainingHelperNextInputs/cond/GatherNd_1/Switch_grad/cond_grad/Merge_2:0\", shape=(2,), dtype=int32))) to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
            "  \"shape. This may consume a large amount of memory.\" % value)\n",
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/indexed_slices.py:446: UserWarning: Converting sparse IndexedSlices(IndexedSlices(indices=Tensor(\"gradients/core_decoder_12/decoder/while/BasicDecoderStep/ScheduledOutputTrainingHelperNextInputs/cond/GatherNd_1/Switch_grad/cond_grad/Merge_1:0\", shape=(?,), dtype=int32), values=Tensor(\"gradients/core_decoder_12/decoder/while/BasicDecoderStep/ScheduledOutputTrainingHelperNextInputs/cond/GatherNd_1/Switch_grad/cond_grad/Merge:0\", shape=(?, 768), dtype=float32), dense_shape=Tensor(\"gradients/core_decoder_12/decoder/while/BasicDecoderStep/ScheduledOutputTrainingHelperNextInputs/cond/GatherNd_1/Switch_grad/cond_grad/Merge_2:0\", shape=(2,), dtype=int32))) to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
            "  \"shape. This may consume a large amount of memory.\" % value)\n",
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/indexed_slices.py:446: UserWarning: Converting sparse IndexedSlices(IndexedSlices(indices=Tensor(\"gradients/core_decoder_13/decoder/while/BasicDecoderStep/ScheduledOutputTrainingHelperNextInputs/cond/GatherNd_1/Switch_grad/cond_grad/Merge_1:0\", shape=(?,), dtype=int32), values=Tensor(\"gradients/core_decoder_13/decoder/while/BasicDecoderStep/ScheduledOutputTrainingHelperNextInputs/cond/GatherNd_1/Switch_grad/cond_grad/Merge:0\", shape=(?, 768), dtype=float32), dense_shape=Tensor(\"gradients/core_decoder_13/decoder/while/BasicDecoderStep/ScheduledOutputTrainingHelperNextInputs/cond/GatherNd_1/Switch_grad/cond_grad/Merge_2:0\", shape=(2,), dtype=int32))) to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
            "  \"shape. This may consume a large amount of memory.\" % value)\n",
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/indexed_slices.py:446: UserWarning: Converting sparse IndexedSlices(IndexedSlices(indices=Tensor(\"gradients/core_decoder_14/decoder/while/BasicDecoderStep/ScheduledOutputTrainingHelperNextInputs/cond/GatherNd_1/Switch_grad/cond_grad/Merge_1:0\", shape=(?,), dtype=int32), values=Tensor(\"gradients/core_decoder_14/decoder/while/BasicDecoderStep/ScheduledOutputTrainingHelperNextInputs/cond/GatherNd_1/Switch_grad/cond_grad/Merge:0\", shape=(?, 768), dtype=float32), dense_shape=Tensor(\"gradients/core_decoder_14/decoder/while/BasicDecoderStep/ScheduledOutputTrainingHelperNextInputs/cond/GatherNd_1/Switch_grad/cond_grad/Merge_2:0\", shape=(2,), dtype=int32))) to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
            "  \"shape. This may consume a large amount of memory.\" % value)\n",
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/indexed_slices.py:446: UserWarning: Converting sparse IndexedSlices(IndexedSlices(indices=Tensor(\"gradients/core_decoder_15/decoder/while/BasicDecoderStep/ScheduledOutputTrainingHelperNextInputs/cond/GatherNd_1/Switch_grad/cond_grad/Merge_1:0\", shape=(?,), dtype=int32), values=Tensor(\"gradients/core_decoder_15/decoder/while/BasicDecoderStep/ScheduledOutputTrainingHelperNextInputs/cond/GatherNd_1/Switch_grad/cond_grad/Merge:0\", shape=(?, 768), dtype=float32), dense_shape=Tensor(\"gradients/core_decoder_15/decoder/while/BasicDecoderStep/ScheduledOutputTrainingHelperNextInputs/cond/GatherNd_1/Switch_grad/cond_grad/Merge_2:0\", shape=(2,), dtype=int32))) to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
            "  \"shape. This may consume a large amount of memory.\" % value)\n",
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/indexed_slices.py:446: UserWarning: Converting sparse IndexedSlices(IndexedSlices(indices=Tensor(\"gradients/core_decoder_16/decoder/while/BasicDecoderStep/ScheduledOutputTrainingHelperNextInputs/cond/GatherNd_1/Switch_grad/cond_grad/Merge_1:0\", shape=(?,), dtype=int32), values=Tensor(\"gradients/core_decoder_16/decoder/while/BasicDecoderStep/ScheduledOutputTrainingHelperNextInputs/cond/GatherNd_1/Switch_grad/cond_grad/Merge:0\", shape=(?, 768), dtype=float32), dense_shape=Tensor(\"gradients/core_decoder_16/decoder/while/BasicDecoderStep/ScheduledOutputTrainingHelperNextInputs/cond/GatherNd_1/Switch_grad/cond_grad/Merge_2:0\", shape=(2,), dtype=int32))) to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
            "  \"shape. This may consume a large amount of memory.\" % value)\n",
            "INFO:tensorflow:Create CheckpointSaverHook.\n",
            "I0605 13:39:14.875878 140255853238144 basic_session_run_hooks.py:558] Create CheckpointSaverHook.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow/python/training/training_util.py:397: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.\n",
            "W0605 13:39:33.645797 140255853238144 deprecation.py:343] From /usr/local/lib/python3.7/dist-packages/tensorflow/python/training/training_util.py:397: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.\n",
            "INFO:tensorflow:Graph was finalized.\n",
            "I0605 13:39:33.806483 140255853238144 monitored_session.py:243] Graph was finalized.\n",
            "INFO:tensorflow:Running local_init_op.\n",
            "I0605 13:39:42.648538 140255853238144 session_manager.py:527] Running local_init_op.\n",
            "INFO:tensorflow:Done running local_init_op.\n",
            "I0605 13:39:43.503517 140255853238144 session_manager.py:530] Done running local_init_op.\n",
            "INFO:tensorflow:Calling checkpoint listeners before saving checkpoint 0...\n",
            "I0605 13:40:49.037795 140255853238144 basic_session_run_hooks.py:629] Calling checkpoint listeners before saving checkpoint 0...\n",
            "INFO:tensorflow:Saving checkpoints for 0 into /content/checkpoints/drum_4bar/train/model.ckpt.\n",
            "I0605 13:40:49.038107 140255853238144 basic_session_run_hooks.py:633] Saving checkpoints for 0 into /content/checkpoints/drum_4bar/train/model.ckpt.\n",
            "INFO:tensorflow:Calling checkpoint listeners after saving checkpoint 0...\n",
            "I0605 13:41:00.477267 140255853238144 basic_session_run_hooks.py:641] Calling checkpoint listeners after saving checkpoint 0...\n",
            "INFO:tensorflow:Calling checkpoint listeners before saving checkpoint 1...\n",
            "I0605 13:43:47.853694 140255853238144 basic_session_run_hooks.py:629] Calling checkpoint listeners before saving checkpoint 1...\n",
            "INFO:tensorflow:Saving checkpoints for 1 into /content/checkpoints/drum_4bar/train/model.ckpt.\n",
            "I0605 13:43:47.853935 140255853238144 basic_session_run_hooks.py:633] Saving checkpoints for 1 into /content/checkpoints/drum_4bar/train/model.ckpt.\n",
            "INFO:tensorflow:Calling checkpoint listeners after saving checkpoint 1...\n",
            "I0605 13:43:58.344229 140255853238144 basic_session_run_hooks.py:641] Calling checkpoint listeners after saving checkpoint 1...\n",
            "INFO:tensorflow:global_step = 1, loss = 399.2965\n",
            "I0605 13:43:58.344651 140255853238144 basic_session_run_hooks.py:266] global_step = 1, loss = 399.2965\n",
            "INFO:tensorflow:Calling checkpoint listeners before saving checkpoint 2...\n",
            "I0605 13:46:04.346544 140255853238144 basic_session_run_hooks.py:629] Calling checkpoint listeners before saving checkpoint 2...\n",
            "INFO:tensorflow:Saving checkpoints for 2 into /content/checkpoints/drum_4bar/train/model.ckpt.\n",
            "I0605 13:46:04.347095 140255853238144 basic_session_run_hooks.py:633] Saving checkpoints for 2 into /content/checkpoints/drum_4bar/train/model.ckpt.\n",
            "INFO:tensorflow:Calling checkpoint listeners after saving checkpoint 2...\n",
            "I0605 13:46:15.657280 140255853238144 basic_session_run_hooks.py:641] Calling checkpoint listeners after saving checkpoint 2...\n",
            "INFO:tensorflow:Calling checkpoint listeners before saving checkpoint 4...\n",
            "I0605 13:47:32.136510 140255853238144 basic_session_run_hooks.py:629] Calling checkpoint listeners before saving checkpoint 4...\n",
            "INFO:tensorflow:Saving checkpoints for 4 into /content/checkpoints/drum_4bar/train/model.ckpt.\n",
            "I0605 13:47:32.136766 140255853238144 basic_session_run_hooks.py:633] Saving checkpoints for 4 into /content/checkpoints/drum_4bar/train/model.ckpt.\n",
            "INFO:tensorflow:Calling checkpoint listeners after saving checkpoint 4...\n",
            "I0605 13:47:43.192105 140255853238144 basic_session_run_hooks.py:641] Calling checkpoint listeners after saving checkpoint 4...\n",
            "INFO:tensorflow:Calling checkpoint listeners before saving checkpoint 6...\n",
            "I0605 13:48:58.656075 140255853238144 basic_session_run_hooks.py:629] Calling checkpoint listeners before saving checkpoint 6...\n",
            "INFO:tensorflow:Saving checkpoints for 6 into /content/checkpoints/drum_4bar/train/model.ckpt.\n",
            "I0605 13:48:58.656327 140255853238144 basic_session_run_hooks.py:633] Saving checkpoints for 6 into /content/checkpoints/drum_4bar/train/model.ckpt.\n",
            "INFO:tensorflow:Calling checkpoint listeners after saving checkpoint 6...\n",
            "I0605 13:49:09.599721 140255853238144 basic_session_run_hooks.py:641] Calling checkpoint listeners after saving checkpoint 6...\n",
            "INFO:tensorflow:Calling checkpoint listeners before saving checkpoint 8...\n",
            "I0605 13:50:24.865970 140255853238144 basic_session_run_hooks.py:629] Calling checkpoint listeners before saving checkpoint 8...\n",
            "INFO:tensorflow:Saving checkpoints for 8 into /content/checkpoints/drum_4bar/train/model.ckpt.\n",
            "I0605 13:50:24.866238 140255853238144 basic_session_run_hooks.py:633] Saving checkpoints for 8 into /content/checkpoints/drum_4bar/train/model.ckpt.\n",
            "INFO:tensorflow:Calling checkpoint listeners after saving checkpoint 8...\n",
            "I0605 13:50:36.753685 140255853238144 basic_session_run_hooks.py:641] Calling checkpoint listeners after saving checkpoint 8...\n",
            "INFO:tensorflow:Calling checkpoint listeners before saving checkpoint 10...\n",
            "I0605 13:51:51.927067 140255853238144 basic_session_run_hooks.py:629] Calling checkpoint listeners before saving checkpoint 10...\n",
            "INFO:tensorflow:Saving checkpoints for 10 into /content/checkpoints/drum_4bar/train/model.ckpt.\n",
            "I0605 13:51:51.927295 140255853238144 basic_session_run_hooks.py:633] Saving checkpoints for 10 into /content/checkpoints/drum_4bar/train/model.ckpt.\n",
            "INFO:tensorflow:Calling checkpoint listeners after saving checkpoint 10...\n",
            "I0605 13:52:02.707483 140255853238144 basic_session_run_hooks.py:641] Calling checkpoint listeners after saving checkpoint 10...\n"
          ]
        }
      ],
      "source": [
        "!python3 /content/magenta/magenta/models/music_vae/music_vae_train.py \\\n",
        " --config=hierdec-drums_4bar \\\n",
        " --run_dir=/content/checkpoints/drum_4bar \\\n",
        " --num_steps=10 \\\n",
        " --mode=train \\\n",
        " --tfds_name=groove/full-midionly \\"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zs1idri7B2HZ"
      },
      "source": [
        "## 샘플 생성 후 재생\n",
        "학습한 모델을 이용해 4마디 드럼 연주 샘플을 생성하고, 재생, 다운로드합니다.  \n",
        "\n",
        "### 모델 불러오기"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "qVz649JuXA5H"
      },
      "outputs": [],
      "source": [
        "os.chdir('/content/magenta/magenta/models/music_vae/')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2SHmOkrjoXUd",
        "outputId": "2e9ea1b5-50cc-4741-e274-23b819e1ea7f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Initializing Music VAE...\n",
            "INFO:tensorflow:Building MusicVAE model with BidirectionalLstmEncoder, HierarchicalLstmDecoder, and hparams:\n",
            "{'max_seq_len': 64, 'z_size': 256, 'free_bits': 48, 'max_beta': 0.2, 'beta_rate': 0.0, 'batch_size': 4, 'grad_clip': 1.0, 'clip_mode': 'global_norm', 'grad_norm_clip_to_zero': 10000, 'learning_rate': 0.001, 'decay_rate': 0.9999, 'min_learning_rate': 1e-05, 'conditional': True, 'dec_rnn_size': [256, 256], 'enc_rnn_size': [512, 512], 'dropout_keep_prob': 1.0, 'sampling_schedule': 'inverse_sigmoid', 'sampling_rate': 1000, 'use_cudnn': False, 'residual_encoder': False, 'residual_decoder': False, 'control_preprocessing_rnn_size': [256]}\n",
            "INFO:tensorflow:\n",
            "Encoder Cells (bidirectional):\n",
            "  units: [512, 512]\n",
            "\n",
            "WARNING:tensorflow:`tf.nn.rnn_cell.MultiRNNCell` is deprecated. This class is equivalent as `tf.keras.layers.StackedRNNCells`, and will be replaced by that in Tensorflow 2.0.\n",
            "WARNING:tensorflow:`tf.nn.rnn_cell.MultiRNNCell` is deprecated. This class is equivalent as `tf.keras.layers.StackedRNNCells`, and will be replaced by that in Tensorflow 2.0.\n",
            "WARNING:tensorflow:`tf.nn.rnn_cell.MultiRNNCell` is deprecated. This class is equivalent as `tf.keras.layers.StackedRNNCells`, and will be replaced by that in Tensorflow 2.0.\n",
            "WARNING:tensorflow:`tf.nn.rnn_cell.MultiRNNCell` is deprecated. This class is equivalent as `tf.keras.layers.StackedRNNCells`, and will be replaced by that in Tensorflow 2.0.\n",
            "INFO:tensorflow:\n",
            "Hierarchical Decoder:\n",
            "  input length: 64\n",
            "  level output lengths: [16, 4]\n",
            "\n",
            "WARNING:tensorflow:`tf.nn.rnn_cell.MultiRNNCell` is deprecated. This class is equivalent as `tf.keras.layers.StackedRNNCells`, and will be replaced by that in Tensorflow 2.0.\n",
            "INFO:tensorflow:\n",
            "Decoder Cells:\n",
            "  units: [256, 256]\n",
            "\n",
            "WARNING:tensorflow:Setting non-training sampling schedule from inverse_sigmoid:1000.000000 to constant:1.0.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow/python/util/dispatch.py:1082: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.cast` instead.\n",
            "WARNING:tensorflow:`tf.nn.rnn_cell.MultiRNNCell` is deprecated. This class is equivalent as `tf.keras.layers.StackedRNNCells`, and will be replaced by that in Tensorflow 2.0.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/content/magenta/magenta/models/music_vae/lstm_utils.py:99: UserWarning: `tf.layers.dense` is deprecated and will be removed in a future version. Please use `tf.keras.layers.Dense` instead.\n",
            "  name=name),\n",
            "/usr/local/lib/python3.7/dist-packages/keras/legacy_tf_layers/core.py:261: UserWarning: `layer.apply` is deprecated and will be removed in a future version. Please use `layer.__call__` method instead.\n",
            "  return layer.apply(inputs)\n",
            "/content/magenta/magenta/contrib/rnn.py:750: UserWarning: `layer.add_variable` is deprecated and will be removed in a future version. Please use `layer.add_weight` method instead.\n",
            "  self._names[\"W\"], [input_size + self._num_units, self._num_units * 4])\n",
            "/content/magenta/magenta/contrib/rnn.py:753: UserWarning: `layer.add_variable` is deprecated and will be removed in a future version. Please use `layer.add_weight` method instead.\n",
            "  initializer=tf.constant_initializer(0.0))\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /content/magenta/magenta/contrib/rnn.py:473: bidirectional_dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `keras.layers.Bidirectional(keras.layers.RNN(cell))`, which is equivalent to this API\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow/python/ops/rnn.py:446: dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `keras.layers.RNN(cell)`, which is equivalent to this API\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow_probability/python/bijectors/affine_linear_operator.py:116: LinearOperator.graph_parents (from tensorflow.python.ops.linalg.linear_operator) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Do not call `graph_parents`.\n",
            "INFO:tensorflow:Restoring parameters from /content/checkpoints/drum_4bar/train/model.ckpt-10\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/content/magenta/magenta/models/music_vae/base_model.py:199: UserWarning: `tf.layers.dense` is deprecated and will be removed in a future version. Please use `tf.keras.layers.Dense` instead.\n",
            "  kernel_initializer=tf.random_normal_initializer(stddev=0.001))\n",
            "/content/magenta/magenta/models/music_vae/base_model.py:205: UserWarning: `tf.layers.dense` is deprecated and will be removed in a future version. Please use `tf.keras.layers.Dense` instead.\n",
            "  kernel_initializer=tf.random_normal_initializer(stddev=0.001))\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "🎉 Done!\n"
          ]
        }
      ],
      "source": [
        "import configs\n",
        "def play(note_sequence):\n",
        "  mm.play_sequence(note_sequence, synth=mm.fluidsynth)\n",
        "\n",
        "def download(note_sequence, filename):\n",
        "  mm.sequence_proto_to_midi_file(note_sequence, filename)\n",
        "  files.download(filename)\n",
        "\n",
        "print(\"Initializing Music VAE...\")\n",
        "\n",
        "config = 'hierdec-drums_4bar'\n",
        "model_path = '/content/checkpoints/drum_4bar/train/model.ckpt-10'\n",
        "num_music = 4\n",
        "\n",
        "music_vae = TrainedModel(\n",
        "      configs.CONFIG_MAP[config], \n",
        "      batch_size=num_music, \n",
        "      checkpoint_dir_or_path=model_path)\n",
        "\n",
        "print('🎉 Done!')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 재생\n",
        "샘플을 생성하고 재생합니다.  \n",
        "temperature가 높을수록 랜덤한 연주가 나옵니다.  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 246
        },
        "id": "Hzse6IzlpycV",
        "outputId": "41caefcf-b1a6-422b-e14e-eeb83f5d5460"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div id=\"id_5\"> </div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<div id=\"id_6\"> </div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<div id=\"id_7\"> </div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<div id=\"id_8\"> </div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "temperature = 0.5 #@param {type:\"slider\", min:0.1, max:1.5, step:0.1}\n",
        "drums_samples = music_vae.sample(n=4, length=64, temperature=temperature)\n",
        "for ns in drums_samples:\n",
        "  play(ns)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 파일로 저장"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "ihkwrIjvqaBF",
        "outputId": "821e5359-03f8-437b-d92e-4433f0840714"
      },
      "outputs": [
        {
          "data": {
            "application/javascript": "\n    async function download(id, filename, size) {\n      if (!google.colab.kernel.accessAllowed) {\n        return;\n      }\n      const div = document.createElement('div');\n      const label = document.createElement('label');\n      label.textContent = `Downloading \"${filename}\": `;\n      div.appendChild(label);\n      const progress = document.createElement('progress');\n      progress.max = size;\n      div.appendChild(progress);\n      document.body.appendChild(div);\n\n      const buffers = [];\n      let downloaded = 0;\n\n      const channel = await google.colab.kernel.comms.open(id);\n      // Send a message to notify the kernel that we're ready.\n      channel.send({})\n\n      for await (const message of channel.messages) {\n        // Send a message to notify the kernel that we're ready.\n        channel.send({})\n        if (message.buffers) {\n          for (const buffer of message.buffers) {\n            buffers.push(buffer);\n            downloaded += buffer.byteLength;\n            progress.value = downloaded;\n          }\n        }\n      }\n      const blob = new Blob(buffers, {type: 'application/binary'});\n      const a = document.createElement('a');\n      a.href = window.URL.createObjectURL(blob);\n      a.download = filename;\n      div.appendChild(a);\n      a.click();\n      div.remove();\n    }\n  ",
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/javascript": "download(\"download_80f8b9a3-4033-443e-a959-33b0734a6b99\", \"<magenta.models.music_vae.trained_model.TrainedModel object at 0x7f09ed3bfad0>_sample_0.mid\", 1794)",
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/javascript": "\n    async function download(id, filename, size) {\n      if (!google.colab.kernel.accessAllowed) {\n        return;\n      }\n      const div = document.createElement('div');\n      const label = document.createElement('label');\n      label.textContent = `Downloading \"${filename}\": `;\n      div.appendChild(label);\n      const progress = document.createElement('progress');\n      progress.max = size;\n      div.appendChild(progress);\n      document.body.appendChild(div);\n\n      const buffers = [];\n      let downloaded = 0;\n\n      const channel = await google.colab.kernel.comms.open(id);\n      // Send a message to notify the kernel that we're ready.\n      channel.send({})\n\n      for await (const message of channel.messages) {\n        // Send a message to notify the kernel that we're ready.\n        channel.send({})\n        if (message.buffers) {\n          for (const buffer of message.buffers) {\n            buffers.push(buffer);\n            downloaded += buffer.byteLength;\n            progress.value = downloaded;\n          }\n        }\n      }\n      const blob = new Blob(buffers, {type: 'application/binary'});\n      const a = document.createElement('a');\n      a.href = window.URL.createObjectURL(blob);\n      a.download = filename;\n      div.appendChild(a);\n      a.click();\n      div.remove();\n    }\n  ",
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/javascript": "download(\"download_66efcaf5-0d78-47d3-8101-be1ee30d6e9e\", \"<magenta.models.music_vae.trained_model.TrainedModel object at 0x7f09ed3bfad0>_sample_1.mid\", 1878)",
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/javascript": "\n    async function download(id, filename, size) {\n      if (!google.colab.kernel.accessAllowed) {\n        return;\n      }\n      const div = document.createElement('div');\n      const label = document.createElement('label');\n      label.textContent = `Downloading \"${filename}\": `;\n      div.appendChild(label);\n      const progress = document.createElement('progress');\n      progress.max = size;\n      div.appendChild(progress);\n      document.body.appendChild(div);\n\n      const buffers = [];\n      let downloaded = 0;\n\n      const channel = await google.colab.kernel.comms.open(id);\n      // Send a message to notify the kernel that we're ready.\n      channel.send({})\n\n      for await (const message of channel.messages) {\n        // Send a message to notify the kernel that we're ready.\n        channel.send({})\n        if (message.buffers) {\n          for (const buffer of message.buffers) {\n            buffers.push(buffer);\n            downloaded += buffer.byteLength;\n            progress.value = downloaded;\n          }\n        }\n      }\n      const blob = new Blob(buffers, {type: 'application/binary'});\n      const a = document.createElement('a');\n      a.href = window.URL.createObjectURL(blob);\n      a.download = filename;\n      div.appendChild(a);\n      a.click();\n      div.remove();\n    }\n  ",
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/javascript": "download(\"download_2d39fdf2-2649-47bb-91f5-3dd740dd9e49\", \"<magenta.models.music_vae.trained_model.TrainedModel object at 0x7f09ed3bfad0>_sample_2.mid\", 1794)",
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/javascript": "\n    async function download(id, filename, size) {\n      if (!google.colab.kernel.accessAllowed) {\n        return;\n      }\n      const div = document.createElement('div');\n      const label = document.createElement('label');\n      label.textContent = `Downloading \"${filename}\": `;\n      div.appendChild(label);\n      const progress = document.createElement('progress');\n      progress.max = size;\n      div.appendChild(progress);\n      document.body.appendChild(div);\n\n      const buffers = [];\n      let downloaded = 0;\n\n      const channel = await google.colab.kernel.comms.open(id);\n      // Send a message to notify the kernel that we're ready.\n      channel.send({})\n\n      for await (const message of channel.messages) {\n        // Send a message to notify the kernel that we're ready.\n        channel.send({})\n        if (message.buffers) {\n          for (const buffer of message.buffers) {\n            buffers.push(buffer);\n            downloaded += buffer.byteLength;\n            progress.value = downloaded;\n          }\n        }\n      }\n      const blob = new Blob(buffers, {type: 'application/binary'});\n      const a = document.createElement('a');\n      a.href = window.URL.createObjectURL(blob);\n      a.download = filename;\n      div.appendChild(a);\n      a.click();\n      div.remove();\n    }\n  ",
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/javascript": "download(\"download_c6962678-7ef5-48aa-8ce0-c2467e2f29a7\", \"<magenta.models.music_vae.trained_model.TrainedModel object at 0x7f09ed3bfad0>_sample_3.mid\", 1734)",
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "for i, ns in enumerate(drums_samples):\n",
        "  download(ns, '%s_sample_%d.mid' % (music_vae, i))"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "4bar_drums.ipynb",
      "provenance": []
    },
    "interpreter": {
      "hash": "0e17ea03fbb32132e5671c308166b4379813c9bd4e6a684633c3a9083728c29e"
    },
    "kernelspec": {
      "display_name": "Python 3.7.13 ('stdata')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.7.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
